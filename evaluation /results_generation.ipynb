{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Imports & Setup ────────────────────────────────────────────────────\n",
    "import sys, os\n",
    "import pandas as pd\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.vectorstores.faiss import DistanceStrategy\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from transformers import pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "# for progress bar (optional)\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Configuration ─────────────────────────────────────────────────────\n",
    "DATA_PATH       = \"/Users/basusmac/Desktop/Github Repositories/SmartCandidate-Analyzer-RAG-Based-Resume-Screening/data/main-data/synthetic-resumes.csv\"\n",
    "FAISS_PATH      = \"/Users/basusmac/Desktop/Github Repositories/SmartCandidate-Analyzer-RAG-Based-Resume-Screening/vectorstore\"\n",
    "TESTSET_PATH    = \"/Users/basusmac/Desktop/Github Repositories/SmartCandidate-Analyzer-RAG-Based-Resume-Screening/data/main-data/test-sets/testset-1.csv\"\n",
    "OUTPUT_PATH     = \"/Users/basusmac/Desktop/Github Repositories/SmartCandidate-Analyzer-RAG-Based-Resume-Screening/data/main-data/gpt4-ragfusion/test-results/testres-1.csv\"\n",
    "\n",
    "RAG_K_THRESHOLD = 10\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "# our new local generator\n",
    "HF_MODEL_NAME   = \"gpt2\"            # ← swap for any HF‑hub model you prefer\n",
    "GEN_MAX_LENGTH  = 512\n",
    "GEN_TEMPERATURE = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Load Data & Build ID → Resume Map ─────────────────────────────────\n",
    "documents = pd.read_csv(DATA_PATH)\n",
    "documents[\"ID\"] = documents[\"ID\"].astype(str)\n",
    "id_resume_dict = dict(zip(documents[\"ID\"], documents[\"Resume\"]))\n",
    "\n",
    "test_df        = pd.read_csv(TESTSET_PATH)\n",
    "question_list  = test_df[\"Job Description\"].tolist()\n",
    "ground_truth   = test_df[\"Ground Truth\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Initialize Embeddings & FAISS ─────────────────────────────────────\n",
    "# 1) Embedding model\n",
    "embedder = HuggingFaceEmbeddings(\n",
    "    model_name=EMBEDDING_MODEL,\n",
    "    model_kwargs={\"device\": \"cpu\"},\n",
    ")\n",
    "\n",
    "# 2) FAISS vectorstore\n",
    "vectorstore_db = FAISS.load_local(\n",
    "    FAISS_PATH,\n",
    "    embedder,\n",
    "    distance_strategy=DistanceStrategy.COSINE,\n",
    "    allow_dangerous_deserialization=True,  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "# ─── Initialize Local HF LLM ────────────────────────────────────────────\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=HF_MODEL_NAME,\n",
    "    max_length=GEN_MAX_LENGTH,\n",
    "    temperature=GEN_TEMPERATURE,\n",
    "    # device=0               # uncomment if you have a GPU\n",
    ")\n",
    "generator = pipeline(\n",
    "     \"text-generation\",\n",
    "     model=HF_MODEL_NAME,\n",
    "     # generate up to 256 new tokens (you can bump this if you want longer answers)\n",
    "     max_new_tokens=256,\n",
    "     # if your prompt is longer than the model’s context window, truncate it\n",
    "     truncation=True,\n",
    "     # ensure we have a pad token (GPT‑2 uses the EOS token for padding)\n",
    "     pad_token_id=generator.tokenizer.eos_token_id,\n",
    "     temperature=GEN_TEMPERATURE,\n",
    "     # device=0               # uncomment if you have a GPU\n",
    ")\n",
    "llm = HuggingFacePipeline(pipeline=generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── RAG Helper Functions ────────────────────────────────────────────────\n",
    "def generate_subquestions(llm, question: str, max_subqs=4) -> list[str]:\n",
    "    prompt = f\"\"\"\n",
    "You are an expert in talent acquisition. Split this job description into {max_subqs} targeted sub-queries,\n",
    "each on its own line. Only use info from the original; don’t make up new requirements.\n",
    "\n",
    "Job Description:\n",
    "{question}\n",
    "\n",
    "Sub-queries:\n",
    "\"\"\"\n",
    "    out = llm(prompt).strip()\n",
    "    return [line for line in out.splitlines() if line.strip()]\n",
    "\n",
    "def reciprocal_rank_fusion(ranks: list[dict[str, float]], k=50) -> dict[str, float]:\n",
    "    fused = {}\n",
    "    for rank_list in ranks:\n",
    "        for idx, (doc_id, score) in enumerate(rank_list.items()):\n",
    "            fused.setdefault(doc_id, 0.0)\n",
    "            fused[doc_id] += 1.0 / (idx + k)\n",
    "    return dict(sorted(fused.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "def retrieve_id_and_rerank(queries: list[str], top_k=RAG_K_THRESHOLD) -> dict[str, float]:\n",
    "    ranklists = []\n",
    "    for q in queries:\n",
    "        docs_with_score = vectorstore_db.similarity_search_with_score(q, k=top_k)\n",
    "        ranklists.append({ str(d.metadata[\"ID\"]): sc for d, sc in docs_with_score })\n",
    "    return reciprocal_rank_fusion(ranklists)\n",
    "\n",
    "def retrieve_documents_with_id(id_scores: dict[str, float], threshold=5) -> list[str]:\n",
    "    top_ids = sorted(id_scores, key=id_scores.get, reverse=True)[:threshold]\n",
    "    return [f\"Applicant ID {i}\\n{id_resume_dict[i]}\" for i in top_ids]\n",
    "\n",
    "def generate_response(llm, question: str, docs: list[str]) -> str:\n",
    "    context = \"\\n\\n\".join(docs)\n",
    "    prompt = f\"\"\"\n",
    "You are an expert in talent acquisition helping pick the best candidate. Use only Applicant IDs to refer to resumes.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer with your selection and reasoning:\n",
    "\"\"\"\n",
    "    return llm(prompt).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running RAG:   0%|          | 0/100 [00:00<?, ?it/s]This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (1024). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n",
      "Running RAG: 100%|██████████| 100/100 [13:44<00:00,  8.24s/it]\n"
     ]
    }
   ],
   "source": [
    "# ─── Run the Full RAG Pipeline ──────────────────────────────────────────\n",
    "results = []\n",
    "for q in tqdm(question_list, desc=\"Running RAG\"):\n",
    "    # 1) build sub‑queries\n",
    "    subs = generate_subquestions(llm, q)\n",
    "    # 2) retrieve & fusion\n",
    "    id_scores = retrieve_id_and_rerank([q] + subs)\n",
    "    docs      = retrieve_documents_with_id(id_scores)\n",
    "    # 3) final answer\n",
    "    ans       = generate_response(llm, q, docs)\n",
    "\n",
    "    results.append({\n",
    "        \"question\":     q,\n",
    "        \"ground_truth\": ground_truth[question_list.index(q)],\n",
    "        \"answer\":       ans,\n",
    "        \"contexts\":     \"===\\n\".join(docs)\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished! Results written to /Users/basusmac/Desktop/Github Repositories/SmartCandidate-Analyzer-RAG-Based-Resume-Screening/data/main-data/gpt4-ragfusion/test-results/testres-1.csv\n"
     ]
    }
   ],
   "source": [
    "# ─── Save to CSV ─────────────────────────────────────────────────────────\n",
    "out_df = pd.DataFrame(results)\n",
    "out_df.to_csv(OUTPUT_PATH, index=False)\n",
    "print(\"✅ Finished! Results written to\", OUTPUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
